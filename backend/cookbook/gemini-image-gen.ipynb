{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini Image Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports and env vars loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_with_streaming(prompt: str):\n",
    "    \"\"\"\n",
    "    Generate image with streaming text first, then yield the image\n",
    "    Fixed: Use regular text model for streaming, then image model for generation\n",
    "    \"\"\"\n",
    "    print(\"ðŸŽ¨ Starting image generation...\")\n",
    "    print(\"ðŸ“ Generating description...\")\n",
    "    \n",
    "    # First stream the text description using regular Gemini model\n",
    "    text_response = \"\"\n",
    "    description_prompt = f\"Describe in detail what a 3D rendered image should look like for this request: {prompt}\"\n",
    "    \n",
    "    stream = client.models.generate_content_stream(\n",
    "        model=\"gemini-2.0-flash\",  # Use regular model for text streaming\n",
    "        contents=description_prompt\n",
    "    )\n",
    "    \n",
    "    print(\"\\nðŸ’­ AI Description:\")\n",
    "    for chunk in stream:\n",
    "        if chunk.text:\n",
    "            text_response += chunk.text\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\nðŸ–¼ï¸ Now generating image based on description...\")\n",
    "    \n",
    "    # Then generate the actual image using the enhanced prompt\n",
    "    enhanced_prompt = f\"{prompt}. Detailed vision: {text_response}\"\n",
    "    \n",
    "    image_response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-preview-image-generation\",\n",
    "        contents=enhanced_prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_modalities=['IMAGE', 'TEXT']  # Both required for image model\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Extract and save the image\n",
    "    for part in image_response.candidates[0].content.parts:\n",
    "        if part.inline_data is not None:\n",
    "            image = Image.open(BytesIO(part.inline_data.data))\n",
    "            filename = 'gemini-streamed-image.png'\n",
    "            image.save(filename)\n",
    "            print(f\"âœ… Image saved as {filename}\")\n",
    "            image.show()\n",
    "            return image, text_response\n",
    "    \n",
    "    return None, text_response\n",
    "\n",
    "# Example usage\n",
    "prompt = 'Hi, can you create a 3d rendered image of Madurai Meenakshi Amman with Lord Shiva to the right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¨ Starting image generation...\n",
      "ðŸ“ Generating description...\n",
      "\n",
      "ðŸ’­ AI Description:\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'The requested combination of response modalities is not supported by the model. models/gemini-2.0-flash-preview-image-generation accepts the following combination of response modalities:\\n* IMAGE, TEXT\\n', 'status': 'INVALID_ARGUMENT'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the streaming image generation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m image, description = \u001b[43mgenerate_image_with_streaming\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mgenerate_image_with_streaming\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m     10\u001b[39m stream = client.models.generate_content_stream(\n\u001b[32m     11\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgemini-2.0-flash-preview-image-generation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     contents=prompt,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     )\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ’­ AI Description:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_response\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code-machine/Gen AI/homework-app/ai-chat-app/venv/lib/python3.11/site-packages/google/genai/models.py:6130\u001b[39m, in \u001b[36mModels.generate_content_stream\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   6124\u001b[39m function_map = _extra_utils.get_function_map(parsed_config)\n\u001b[32m   6126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m1\u001b[39m:\n\u001b[32m   6127\u001b[39m   \u001b[38;5;66;03m# First request gets a function call.\u001b[39;00m\n\u001b[32m   6128\u001b[39m   \u001b[38;5;66;03m# Then get function response parts.\u001b[39;00m\n\u001b[32m   6129\u001b[39m   \u001b[38;5;66;03m# Yield chunks only if there's no function response parts.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6130\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   6131\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunction_map\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   6132\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code-machine/Gen AI/homework-app/ai-chat-app/venv/lib/python3.11/site-packages/google/genai/models.py:5013\u001b[39m, in \u001b[36mModels._generate_content_stream\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5010\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   5011\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m5013\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest_streamed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5014\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   5015\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   5017\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   5018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_GenerateContentResponse_from_vertex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5019\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_dict\u001b[49m\n\u001b[32m   5020\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code-machine/Gen AI/homework-app/ai-chat-app/venv/lib/python3.11/site-packages/google/genai/_api_client.py:804\u001b[39m, in \u001b[36mBaseApiClient.request_streamed\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest_streamed\u001b[39m(\n\u001b[32m    794\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    795\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    798\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    799\u001b[39m ) -> Generator[Any, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[32m    800\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m    801\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m    802\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m804\u001b[39m   session_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    805\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m session_response.segments():\n\u001b[32m    806\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code-machine/Gen AI/homework-app/ai-chat-app/venv/lib/python3.11/site-packages/google/genai/_api_client.py:702\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m    694\u001b[39m   httpx_request = \u001b[38;5;28mself\u001b[39m._httpx_client.build_request(\n\u001b[32m    695\u001b[39m       method=http_request.method,\n\u001b[32m    696\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    699\u001b[39m       timeout=http_request.timeout,\n\u001b[32m    700\u001b[39m   )\n\u001b[32m    701\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.send(httpx_request, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    703\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    704\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    705\u001b[39m   )\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code-machine/Gen AI/homework-app/ai-chat-app/venv/lib/python3.11/site-packages/google/genai/errors.py:101\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m     99\u001b[39m status_code = response.status_code\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    103\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'The requested combination of response modalities is not supported by the model. models/gemini-2.0-flash-preview-image-generation accepts the following combination of response modalities:\\n* IMAGE, TEXT\\n', 'status': 'INVALID_ARGUMENT'}}"
     ]
    }
   ],
   "source": [
    "# Run the streaming image generation\n",
    "image, description = generate_image_with_streaming(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_for_ui(prompt: str):\n",
    "    \"\"\"\n",
    "    Generator function that yields text chunks first, then the final image\n",
    "    Perfect for web UI integration with real-time updates\n",
    "    Fixed: Use regular text model for streaming, then image model\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    \n",
    "    yield {\"type\": \"status\", \"message\": \"ðŸ–¼ï¸ Now generating image...\"}\n",
    "    \n",
    "    # Generate image using the enhanced prompt\n",
    "    \n",
    "    image_response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-preview-image-generation\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_modalities=['IMAGE', 'TEXT']  # Both required for image model\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Extract and yield image\n",
    "    for part in image_response.candidates[0].content.parts:\n",
    "        if part.inline_data is not None:\n",
    "            # Convert to base64 for web transfer\n",
    "            image_data = part.inline_data.data\n",
    "            image_b64 = base64.b64encode(image_data).decode('utf-8')\n",
    "            \n",
    "            # Also save locally\n",
    "            image = Image.open(BytesIO(image_data))\n",
    "            filename = f'gemini-ui-image-{int(time.time())}.png'\n",
    "            image.save(filename)\n",
    "            \n",
    "            yield {\n",
    "                \"type\": \"image_complete\",\n",
    "                \"image_b64\": image_b64,\n",
    "                \"filename\": filename,\n",
    "                \"message\": f\"âœ… Image generated and saved as {filename}\"\n",
    "            }\n",
    "            break\n",
    "    \n",
    "    yield {\"type\": \"complete\", \"message\": \"ðŸŽ‰ Generation complete!\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UI-Friendly Streaming Demo ===\n",
      "\n",
      "âœ… Image generated and saved as gemini-ui-image-1749421819.png\n",
      "ðŸ“Š Image size: 1304616 base64 chars\n",
      "\n",
      "ðŸŽ‰ Generation complete!\n"
     ]
    }
   ],
   "source": [
    "# Demo the UI-friendly generator\n",
    "print(\"=== UI-Friendly Streaming Demo ===\")\n",
    "\n",
    "for update in generate_image_for_ui(prompt):\n",
    "    if update[\"type\"] == \"image_complete\":\n",
    "        print(f\"\\n{update['message']}\")\n",
    "        print(f\"ðŸ“Š Image size: {len(update['image_b64'])} base64 chars\")\n",
    "        # Display the image\n",
    "        image_data = base64.b64decode(update['image_b64'])\n",
    "        image = Image.open(BytesIO(image_data))\n",
    "        image.show()\n",
    "    elif update[\"type\"] == \"complete\":\n",
    "        print(f\"\\n{update['message']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
